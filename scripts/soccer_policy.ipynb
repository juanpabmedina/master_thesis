{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linprog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_policy(s, Q, agent_actions, opponent_actions):\n",
    "    \"\"\"\n",
    "    Compute the policy π[s, .] using the given Q-function, considering opponent actions.\n",
    "\n",
    "    Args:\n",
    "        s (int): The current state.\n",
    "        Q (dict): A nested dictionary Q[s][a][o], where:\n",
    "                  - Q[s][a][o] gives the Q-value for state s, action a, and opponent action o.\n",
    "        agent_actions (list): List of possible actions for the agent.\n",
    "        opponent_actions (list): List of possible actions for the opponent.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary π[s, a] representing the optimal policy for state s.\n",
    "    \"\"\"\n",
    "    best_policy = None\n",
    "    max_value = float('-inf')\n",
    "    \n",
    "    # Iterate over candidate policies π'\n",
    "    for candidate_policy in generate_candidate_policies(agent_actions):\n",
    "        # Calculate the worst-case value (min over opponent actions)\n",
    "        worst_case_value = float('inf')\n",
    "        \n",
    "        for o in opponent_actions:\n",
    "            # Calculate the expected value for this opponent action o\n",
    "            expected_value = 0\n",
    "            for a in agent_actions:\n",
    "                expected_value += candidate_policy[a] * Q[s][a][o]\n",
    "            \n",
    "            # Update the worst-case (minimum over opponent actions)\n",
    "            worst_case_value = min(worst_case_value, expected_value)\n",
    "        \n",
    "        # Check if this policy maximizes the worst-case value\n",
    "        if worst_case_value > max_value:\n",
    "            max_value = worst_case_value\n",
    "            best_policy = candidate_policy\n",
    "    \n",
    "    return best_policy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_candidate_policies(actions):\n",
    "    \"\"\"\n",
    "    Generate all candidate policies (distributions over actions).\n",
    "    This assumes discrete actions and returns deterministic policies.\n",
    "\n",
    "    Args:\n",
    "        actions (list): List of possible actions.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries representing policies.\n",
    "    \"\"\"\n",
    "    policies = []\n",
    "    num_actions = len(actions)\n",
    "    \n",
    "    # Generate simple deterministic policies\n",
    "    for i in range(num_actions):\n",
    "        policy = {a: 0.0 for a in actions}\n",
    "        policy[actions[i]] = 1.0\n",
    "        policies.append(policy)\n",
    "    \n",
    "    return policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_policy_lp(s, Q, agent_actions, opponent_actions):\n",
    "    \"\"\"\n",
    "    Compute the optimal policy π[s, .] using linear programming.\n",
    "\n",
    "    Args:\n",
    "        s (int): The current state.\n",
    "        Q (dict): A nested dictionary Q[s][a][o], where:\n",
    "                  - Q[s][a][o] gives the Q-value for state s, action a, and opponent action o.\n",
    "        agent_actions (list): List of possible actions for the agent.\n",
    "        opponent_actions (list): List of possible actions for the opponent.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary π[s, a] representing the optimal policy for state s.\n",
    "    \"\"\"\n",
    "    num_actions = len(agent_actions)\n",
    "    num_opponent_actions = len(opponent_actions)\n",
    "    \n",
    "    # Decision variables: π[a] for each agent action, plus v (worst-case value)\n",
    "    # Total decision variables = num_actions + 1\n",
    "    c = [-1] + [0] * num_actions  # Objective: Maximize v (negative for linprog minimization)\n",
    "    \n",
    "    # Constraints: Gx <= h\n",
    "    G = []\n",
    "    h = []\n",
    "    \n",
    "    # Add constraints for worst-case expected value\n",
    "    for o in opponent_actions:\n",
    "        row = [1]  # Coefficient for v\n",
    "        for a in agent_actions:\n",
    "            row.append(-Q[s][a][o])  # Coefficients for -π[a] * Q[s, a, o]\n",
    "        G.append(row)\n",
    "        h.append(0)  # Constraint: v <= sum(π[a] * Q[s, a, o])\n",
    "    \n",
    "    # Add constraints for probability distribution\n",
    "    # Sum of π[a] = 1\n",
    "    equality_row = [0] + [1] * num_actions\n",
    "    A_eq = [equality_row]\n",
    "    b_eq = [1]\n",
    "    \n",
    "    # π[a] >= 0 for all a\n",
    "    for i in range(num_actions):\n",
    "        constraint = [0] + [-1 if j == i else 0 for j in range(num_actions)]\n",
    "        G.append(constraint)\n",
    "        h.append(0)\n",
    "    \n",
    "    # Solve the linear program\n",
    "    result = linprog(c, A_ub=G, b_ub=h, A_eq=A_eq, b_eq=b_eq, bounds=(None, None), method='highs')\n",
    "    \n",
    "    if result.success:\n",
    "        # Extract policy from result\n",
    "        policy = {a: prob for a, prob in zip(agent_actions, result.x[1:])}\n",
    "        return policy\n",
    "    else:\n",
    "        raise ValueError(\"Linear programming failed to find a solution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy for state 0: {'N': 0.5384615384615384, 'S': -0.0, 'E': 0.3846153846153846, 'W': -0.0, 'Stand': 0.07692307692307693}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define Q-values: Q[s][a][o] = reward for (state, action, opponent action)\n",
    "    Q = {\n",
    "        0: {  # State 0\n",
    "            'N': {'N': 1, 'S': -1, 'E': 0, 'W': 2, 'Stand': 3},\n",
    "            'S': {'N': 0, 'S': 2, 'E': -2, 'W': 1, 'Stand': -1},\n",
    "            'E': {'N': -1, 'S': 3, 'E': 1, 'W': -2, 'Stand': 0},\n",
    "            'W': {'N': 2, 'S': 1, 'E': -3, 'W': 0, 'Stand': 4},\n",
    "            'Stand': {'N': 3, 'S': -2, 'E': 0, 'W': 1, 'Stand': 2},\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Define possible actions for the agent and opponent\n",
    "    actions = ['N', 'S', 'E', 'W', 'Stand']\n",
    "\n",
    "    # Compute the policy for state 0\n",
    "    state = 0\n",
    "    optimal_policy = compute_policy(state, Q, actions, actions)\n",
    "    optimal_policy = compute_policy_lp(state, Q, actions, actions)\n",
    "    print(f\"Optimal policy for state {state}: {optimal_policy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5384615384615384"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_policy['N']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marl_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
