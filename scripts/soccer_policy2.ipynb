{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linprog\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialQAgent:\n",
    "    def __init__(self, states, actions, opponent_actions, alpha=0.1, gamma=0.9, decay=0.99):\n",
    "        \"\"\"\n",
    "        Initialize the Adversarial Q-Learning agent.\n",
    "\n",
    "        Args:\n",
    "            states (list): List of possible states.\n",
    "            actions (list): List of possible actions for the agent.\n",
    "            opponent_actions (list): List of possible actions for the opponent.\n",
    "            alpha (float): Learning rate.\n",
    "            gamma (float): Discount factor.\n",
    "            decay (float): Learning rate decay.\n",
    "        \"\"\"\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.opponent_actions = opponent_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.decay = decay\n",
    "        \n",
    "        # Initialize Q-table and V-table\n",
    "        self.Q = {s: {a: {o: 0.0 for o in opponent_actions} for a in actions} for s in states}\n",
    "        self.V = {s: 0.0 for s in states}\n",
    "        self.policy = {s: {a: 1.0 / len(actions) for a in actions} for s in states}  # Initialize uniform policy\n",
    "\n",
    "    def update_q_value(self, s, a, o, s_prime, reward):\n",
    "        \"\"\"\n",
    "        Update the Q-value based on the reward and next state value.\n",
    "\n",
    "        Args:\n",
    "            s (int): Current state.\n",
    "            a (str): Action taken by the agent.\n",
    "            o (str): Action taken by the opponent.\n",
    "            s_prime (int): Next state.\n",
    "            reward (float): Received reward.\n",
    "        \"\"\"\n",
    "        self.Q[s][a][o] = (1 - self.alpha) * self.Q[s][a][o] + \\\n",
    "                          self.alpha * (reward + self.gamma * self.V[s_prime])\n",
    "\n",
    "    def compute_policy_lp(self, s):\n",
    "        \"\"\"\n",
    "        Compute the optimal policy π[s, .] using linear programming.\n",
    "\n",
    "        Args:\n",
    "            s (int): The current state.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary π[s, a] representing the optimal policy for state s.\n",
    "        \"\"\"\n",
    "        num_actions = len(self.actions)\n",
    "        \n",
    "        # Decision variables: π[a] for each action + v (worst-case value)\n",
    "        c = [-1] + [0] * num_actions  # Maximize v (converted to minimization by negation)\n",
    "\n",
    "        # Constraints: Gx <= h\n",
    "        G = []\n",
    "        h = []\n",
    "        \n",
    "        # Worst-case expected value constraint\n",
    "        for o in self.opponent_actions:\n",
    "            row = [1]  # Coefficient for v\n",
    "            for a in self.actions:\n",
    "                row.append(-self.Q[s][a][o])  # Coefficients for -π[a] * Q[s, a, o]\n",
    "            G.append(row)\n",
    "            h.append(0)\n",
    "\n",
    "        # Probability distribution constraint: sum(π[a]) = 1\n",
    "        equality_row = [0] + [1] * num_actions\n",
    "        A_eq = [equality_row]\n",
    "        b_eq = [1]\n",
    "\n",
    "        # π[a] >= 0 for all a\n",
    "        for i in range(num_actions):\n",
    "            constraint = [0] + [-1 if j == i else 0 for j in range(num_actions)]\n",
    "            G.append(constraint)\n",
    "            h.append(0)\n",
    "\n",
    "        # Solve linear program\n",
    "        result = linprog(c, A_ub=G, b_ub=h, A_eq=A_eq, b_eq=b_eq, bounds=(None, None), method='highs')\n",
    "\n",
    "        if result.success:\n",
    "            return {a: prob for a, prob in zip(self.actions, result.x[1:])}\n",
    "        else:\n",
    "            raise ValueError(f\"Linear programming failed at state {s}\")\n",
    "\n",
    "    def update_policy_and_value(self, s):\n",
    "        \"\"\"\n",
    "        Update the policy π[s, .] using linear programming and compute the state value V[s].\n",
    "\n",
    "        Args:\n",
    "            s (int): The state for which to update the policy and value function.\n",
    "        \"\"\"\n",
    "        self.policy[s] = self.compute_policy_lp(s)\n",
    "        \n",
    "        # Compute new V[s] as the worst-case expected reward\n",
    "        min_value = float('inf')\n",
    "        for o in self.opponent_actions:\n",
    "            expected_value = sum(self.policy[s][a] * self.Q[s][a][o] for a in self.actions)\n",
    "            min_value = min(min_value, expected_value)\n",
    "\n",
    "        self.V[s] = min_value\n",
    "\n",
    "    def decay_learning_rate(self):\n",
    "        \"\"\"\n",
    "        Decay the learning rate alpha.\n",
    "        \"\"\"\n",
    "        self.alpha *= self.decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Q-value for (s=0, a=N, o=S): -0.1\n",
      "Optimal policy for state 0: {'N': -0.0, 'S': 1.0, 'E': -0.0, 'W': -0.0, 'Stand': -0.0}\n",
      "Value function V[0]: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    states = [0, 1]  # Example states\n",
    "    actions = ['N', 'S', 'E', 'W', 'Stand']\n",
    "    opponent_actions = ['N', 'S', 'E', 'W', 'Stand']\n",
    "\n",
    "    agent = AdversarialQAgent(states, actions, opponent_actions)\n",
    "\n",
    "    # Simulate an experience\n",
    "    state = 0\n",
    "    action = 'N'\n",
    "    opponent_action = 'S'\n",
    "    next_state = 1\n",
    "    reward = -1\n",
    "\n",
    "    # Update Q-value\n",
    "    agent.update_q_value(state, action, opponent_action, next_state, reward)\n",
    "\n",
    "    # Update policy and value function\n",
    "    agent.update_policy_and_value(state)\n",
    "\n",
    "    # Decay learning rate\n",
    "    agent.decay_learning_rate()\n",
    "\n",
    "    print(f\"Updated Q-value for (s={state}, a={action}, o={opponent_action}): {agent.Q[state][action][opponent_action]}\")\n",
    "    print(f\"Optimal policy for state {state}: {agent.policy[state]}\")\n",
    "    print(f\"Value function V[{state}]: {agent.V[state]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marl_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
