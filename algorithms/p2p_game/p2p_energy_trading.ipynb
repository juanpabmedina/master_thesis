{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from agents import Prosumer\n",
    "from enviroment import P2PEnergyMarket\n",
    "from algorithms import MinimaxQ\n",
    "from trainer import MinimaxTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards(rewards, title, window_size=10):\n",
    "    \"\"\"\n",
    "    Grafica la recompensa por episodio y su media móvil.\n",
    "    \n",
    "    :param rewards: Lista de recompensas obtenidas en cada episodio.\n",
    "    :param window_size: Tamaño de la ventana para el promedio móvil.\n",
    "    \"\"\"\n",
    "    episodes = np.arange(len(rewards))\n",
    "    \n",
    "    # Cálculo del promedio móvil\n",
    "    smoothed_rewards = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(episodes, rewards, label='Recompensa por episodio', alpha=0.5)\n",
    "    plt.plot(episodes[:len(smoothed_rewards)], smoothed_rewards, label=f'Promedio móvil (window={window_size})', color='red', linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Episodios')\n",
    "    plt.ylabel('Recompensa')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1_actions = np.array([-0.2, 0, 0.2])\n",
    "agent2_actions =  np.array([-0.2, 0, 0.2])\n",
    "generation_power = np.round(np.linspace(10, 20, 51),2)\n",
    "demand_power = np.round(np.linspace(10, 50, 201),2)\n",
    "\n",
    "agent_parameters = {\n",
    "    'name': 'A',\n",
    "    'actions': agent1_actions,\n",
    "    'opponent_actions': agent2_actions,\n",
    "    'states': generation_power,\n",
    "    'epsilon': 0.2,\n",
    "    'rol': 'generator'\n",
    "}\n",
    "\n",
    "agent_parameters2 = {\n",
    "    'name': 'B',\n",
    "    'actions': agent2_actions,\n",
    "    'opponent_actions': agent1_actions,\n",
    "    'states': demand_power,\n",
    "    'epsilon': 0.2,\n",
    "    'rol': 'consummer'\n",
    "}\n",
    "\n",
    "agent = Prosumer(agent_parameters)\n",
    "opponent = Prosumer(agent_parameters2)\n",
    "\n",
    "p2pmarket = P2PEnergyMarket()\n",
    "mmq = MinimaxQ(agent)\n",
    "\n",
    "trainer = MinimaxTrainer(agent=agent, opponent=opponent, enviroment=p2pmarket, algorithm=mmq)\n",
    "\n",
    "state = agent.get_state(20)\n",
    "opponent_state = opponent.get_state(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_reward = trainer.train(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Evolución de la recompensa (goles por cada 1000 ts) usando MR vs Random'\n",
    "plot_rewards(hist_reward,title, window_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state:  14.0\n",
      "action:  0.0\n",
      "next state:  14.0\n",
      "oponent state:  10.4\n",
      "opponent action:  0.2\n",
      "next opponent state:  10.6\n",
      "reward:  45.599999999999994\n",
      "policy:  {10.0: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 10.2: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 10.4: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 10.6: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 10.8: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 11.0: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 11.2: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 11.4: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 11.6: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 11.8: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 12.0: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 12.2: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 12.4: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 12.6: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 12.8: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 13.0: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 13.2: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 13.4: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 13.6: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 13.8: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 14.0: {-0.2: 0.0, 0.0: 0.0, 0.2: 1.0}, 14.2: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 14.4: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 14.6: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 14.8: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 15.0: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 15.2: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 15.4: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 15.6: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 15.8: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 16.0: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 16.2: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 16.4: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 16.6: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 16.8: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 17.0: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 17.2: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 17.4: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 17.6: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 17.8: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 18.0: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 18.2: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 18.4: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 18.6: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 18.8: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 19.0: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 19.2: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 19.4: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 19.6: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 19.8: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}, 20.0: {-0.2: 0.3333333333333333, 0.0: 0.3333333333333333, 0.2: 0.3333333333333333}}\n"
     ]
    }
   ],
   "source": [
    "# Agent action and next state\n",
    "action = round(agent.choose_action(state),2)\n",
    "next_state = agent.get_next_state(state, action)\n",
    "print('state: ', state)\n",
    "print('action: ', action)\n",
    "print('next state: ', next_state)\n",
    "\n",
    "# Opponent action and next opponent state\n",
    "opponent_action = opponent.choose_action(opponent_state)\n",
    "next_opponent_state = opponent.get_next_state(opponent_state, opponent_action)\n",
    "print('oponent state: ', opponent_state)\n",
    "print('opponent action: ', opponent_action)\n",
    "print('next opponent state: ', next_opponent_state)\n",
    "\n",
    "# Obtein the reward\n",
    "reward = p2pmarket.get_reward(rol=agent.rol, generator_state=state, consumer_state=opponent_state)\n",
    "print('reward: ', reward)\n",
    "\n",
    "# Calculate the new policy\n",
    "policy = mmq.update(state, action, opponent_action, reward, next_state)\n",
    "print('policy: ', policy)\n",
    "\n",
    "\n",
    "# Update states and policy\n",
    "state = next_state\n",
    "opponent_state = next_opponent_state\n",
    "agent.pi_table = policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marl_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
